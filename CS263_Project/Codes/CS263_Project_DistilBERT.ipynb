{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS263_Project_DistilBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsWUcIFJ8CAp",
        "colab_type": "text"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1A6tOerXST0",
        "colab_type": "code",
        "outputId": "16837286-3df0-4513-cb78-c913abfe01c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/CS263_Project/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CS263_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urxdiTR88JDy",
        "colab_type": "text"
      },
      "source": [
        "Loading all datasets from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcAEcWu6ZolC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(r\"all_data.csv\")\n",
        "test_set = pd.read_csv(r\"test_set.csv\")\n",
        "sentiment_data = pd.read_csv(r\"sentiment.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jVqda3S8ace",
        "colab_type": "code",
        "outputId": "4689bc90-3b57-4341-c78d-06dae1fca67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2ac71177476bee33855145706f24777ef45839c1b77f93ead03191e636764590\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkG7aoqf8hbk",
        "colab_type": "text"
      },
      "source": [
        "Function for BERT Vectorization is defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFk5zLSv8fuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def vectorizeBERT(df, bert_type = \"bert\"):\n",
        "\n",
        "  if bert_type == \"distil\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "  elif bert_type == \"roberta\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
        "  else:\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "  # Load pretrained model/tokenizer\n",
        "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "  model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "  tokenized = df[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "  # padding step\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "  attention_mask.shape\n",
        "\n",
        "  input_ids = torch.tensor(np.array(padded))\n",
        "  attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "  # Slice the output for the first position for all the sequences, take all hidden unit outputs\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "\n",
        "  return features\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxA8eWS_Xd1",
        "colab_type": "text"
      },
      "source": [
        "Function for obtaining sentiment score using sentiment data is defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mof6wXn-z-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pickle\n",
        "def obtain_sentiment_classifier(sentiment_data,data_select=4000,bert_type=\"bert\"):\n",
        "\n",
        "  #Randomly selecting a small portion of sentiment data, since the dataset is too big\n",
        "  rand_ind = random.sample(range(sentiment_data.shape[0]),data_select)\n",
        "  sentiment=sentiment_data.iloc[rand_ind]\n",
        "  # Build the sentiment model\n",
        "  features = vectorizeBERT(sentiment,bert_type=bert_type)\n",
        "  features_ds = pd.DataFrame(features)\n",
        "  labels = sentiment[\"label\"]\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features_ds, labels)\n",
        "  #Logistic Regression classifier for sentiment analysis\n",
        "  lr_clf_sentiment = LogisticRegression(max_iter=5000)\n",
        "  lr_clf_sentiment.fit(train_features, train_labels)\n",
        "  #Calculate score of logistic regression classifier on sentiment data\n",
        "  sent_accuracy = lr_clf_sentiment.score(test_features, test_labels)\n",
        "  print(\"The accuracy of Logistic Regression classifier on sentiment data is: \",sent_accuracy)\n",
        "  pickle.dump(lr_clf_sentiment, open(\"sentiment_model_distil\", 'wb'))\n",
        "  return lr_clf_sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G93tK-j8xh_",
        "colab_type": "code",
        "outputId": "ab6f21fa-34b7-4df4-8638-2f8cd4c311fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Build the cognitive distortion model\n",
        "# M x N matrix\n",
        "import time\n",
        "start_time = time.time()\n",
        "features = vectorizeBERT(df,bert_type=\"distil\")\n",
        "print(\"Time taken is:\",time.time()-start_time)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken is: 91.20670056343079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C47k6CTXAS64",
        "colab_type": "text"
      },
      "source": [
        "Creating training dataset with features and sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaB1h_A58yWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "3d244944-8635-4899-bb2e-00b2b80c6fb5"
      },
      "source": [
        "labels = df[\"label\"]\n",
        "features_df = pd.DataFrame(features)\n",
        "lr_clf_sentiment = obtain_sentiment_classifier(sentiment_data,data_select=4000,bert_type=\"distil\")\n",
        "#lr_clf_sentiment = pickle.load(open(\"sentiment_model_distil\",\"rb\"))\n",
        "sentiment_score = lr_clf_sentiment.predict_log_proba(features)[:,0]\n",
        "features_df[\"sentiment_score\"] = sentiment_score\n",
        "features_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>sentiment_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.087593</td>\n",
              "      <td>-0.014889</td>\n",
              "      <td>-0.047897</td>\n",
              "      <td>-0.194918</td>\n",
              "      <td>-0.207854</td>\n",
              "      <td>-0.149689</td>\n",
              "      <td>0.489435</td>\n",
              "      <td>0.592164</td>\n",
              "      <td>-0.200689</td>\n",
              "      <td>-0.340896</td>\n",
              "      <td>-0.043105</td>\n",
              "      <td>-0.357107</td>\n",
              "      <td>-0.368491</td>\n",
              "      <td>0.486405</td>\n",
              "      <td>0.120391</td>\n",
              "      <td>0.265987</td>\n",
              "      <td>-0.057229</td>\n",
              "      <td>0.173425</td>\n",
              "      <td>0.030791</td>\n",
              "      <td>-0.002168</td>\n",
              "      <td>0.162766</td>\n",
              "      <td>-0.071762</td>\n",
              "      <td>0.113593</td>\n",
              "      <td>-0.003992</td>\n",
              "      <td>0.145600</td>\n",
              "      <td>-0.006519</td>\n",
              "      <td>-0.068760</td>\n",
              "      <td>-0.180929</td>\n",
              "      <td>-0.042907</td>\n",
              "      <td>0.015845</td>\n",
              "      <td>-0.120545</td>\n",
              "      <td>0.224573</td>\n",
              "      <td>-0.039434</td>\n",
              "      <td>-0.022718</td>\n",
              "      <td>-0.027674</td>\n",
              "      <td>-0.125107</td>\n",
              "      <td>-0.223559</td>\n",
              "      <td>-0.179898</td>\n",
              "      <td>-0.010274</td>\n",
              "      <td>0.099481</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.188875</td>\n",
              "      <td>-0.191360</td>\n",
              "      <td>0.430189</td>\n",
              "      <td>-0.233547</td>\n",
              "      <td>-0.274344</td>\n",
              "      <td>0.024809</td>\n",
              "      <td>-0.002019</td>\n",
              "      <td>-0.366574</td>\n",
              "      <td>-0.273057</td>\n",
              "      <td>-0.253475</td>\n",
              "      <td>0.134684</td>\n",
              "      <td>0.250884</td>\n",
              "      <td>0.053999</td>\n",
              "      <td>-0.082660</td>\n",
              "      <td>0.354163</td>\n",
              "      <td>0.376425</td>\n",
              "      <td>0.027355</td>\n",
              "      <td>0.117842</td>\n",
              "      <td>-0.030213</td>\n",
              "      <td>-0.087974</td>\n",
              "      <td>0.074308</td>\n",
              "      <td>-0.033364</td>\n",
              "      <td>0.121252</td>\n",
              "      <td>-5.813449</td>\n",
              "      <td>-0.277602</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>-0.206079</td>\n",
              "      <td>-0.132437</td>\n",
              "      <td>-0.318758</td>\n",
              "      <td>0.039121</td>\n",
              "      <td>-0.214331</td>\n",
              "      <td>-0.049884</td>\n",
              "      <td>-0.111300</td>\n",
              "      <td>0.219482</td>\n",
              "      <td>0.050039</td>\n",
              "      <td>-0.065004</td>\n",
              "      <td>0.061999</td>\n",
              "      <td>0.398687</td>\n",
              "      <td>0.502432</td>\n",
              "      <td>-1.371694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.086113</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.082821</td>\n",
              "      <td>-0.302138</td>\n",
              "      <td>-0.132343</td>\n",
              "      <td>-0.168910</td>\n",
              "      <td>0.385044</td>\n",
              "      <td>0.295699</td>\n",
              "      <td>-0.168847</td>\n",
              "      <td>-0.398763</td>\n",
              "      <td>0.040778</td>\n",
              "      <td>-0.258535</td>\n",
              "      <td>-0.513846</td>\n",
              "      <td>0.402998</td>\n",
              "      <td>0.176287</td>\n",
              "      <td>0.146144</td>\n",
              "      <td>-0.015506</td>\n",
              "      <td>0.139680</td>\n",
              "      <td>0.045123</td>\n",
              "      <td>-0.023466</td>\n",
              "      <td>-0.026054</td>\n",
              "      <td>-0.108402</td>\n",
              "      <td>0.089890</td>\n",
              "      <td>0.193315</td>\n",
              "      <td>-0.016541</td>\n",
              "      <td>0.071388</td>\n",
              "      <td>-0.117635</td>\n",
              "      <td>-0.002586</td>\n",
              "      <td>0.031290</td>\n",
              "      <td>0.025384</td>\n",
              "      <td>0.226995</td>\n",
              "      <td>-0.048654</td>\n",
              "      <td>-0.110353</td>\n",
              "      <td>-0.061637</td>\n",
              "      <td>0.027997</td>\n",
              "      <td>-0.068897</td>\n",
              "      <td>-0.147971</td>\n",
              "      <td>-0.233819</td>\n",
              "      <td>-0.133965</td>\n",
              "      <td>0.109694</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040963</td>\n",
              "      <td>-0.126840</td>\n",
              "      <td>0.488566</td>\n",
              "      <td>-0.280560</td>\n",
              "      <td>-0.421362</td>\n",
              "      <td>0.164499</td>\n",
              "      <td>-0.172866</td>\n",
              "      <td>-0.229673</td>\n",
              "      <td>-0.168436</td>\n",
              "      <td>-0.189583</td>\n",
              "      <td>-0.011746</td>\n",
              "      <td>0.182851</td>\n",
              "      <td>0.010062</td>\n",
              "      <td>-0.117220</td>\n",
              "      <td>0.054786</td>\n",
              "      <td>0.153825</td>\n",
              "      <td>0.072313</td>\n",
              "      <td>0.081903</td>\n",
              "      <td>-0.177966</td>\n",
              "      <td>-0.143666</td>\n",
              "      <td>0.231226</td>\n",
              "      <td>-0.255794</td>\n",
              "      <td>0.090847</td>\n",
              "      <td>-6.920448</td>\n",
              "      <td>-0.151386</td>\n",
              "      <td>0.003124</td>\n",
              "      <td>-0.309502</td>\n",
              "      <td>0.052130</td>\n",
              "      <td>-0.104728</td>\n",
              "      <td>0.029567</td>\n",
              "      <td>-0.131922</td>\n",
              "      <td>0.194381</td>\n",
              "      <td>-0.104370</td>\n",
              "      <td>0.161944</td>\n",
              "      <td>0.087093</td>\n",
              "      <td>-0.084087</td>\n",
              "      <td>-0.019081</td>\n",
              "      <td>0.375111</td>\n",
              "      <td>0.354356</td>\n",
              "      <td>-1.236154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.112473</td>\n",
              "      <td>0.044041</td>\n",
              "      <td>-0.021386</td>\n",
              "      <td>-0.235980</td>\n",
              "      <td>-0.122235</td>\n",
              "      <td>-0.035452</td>\n",
              "      <td>0.343498</td>\n",
              "      <td>0.445589</td>\n",
              "      <td>-0.111061</td>\n",
              "      <td>-0.305136</td>\n",
              "      <td>-0.044966</td>\n",
              "      <td>-0.090133</td>\n",
              "      <td>-0.132127</td>\n",
              "      <td>0.225219</td>\n",
              "      <td>0.072561</td>\n",
              "      <td>0.344129</td>\n",
              "      <td>0.045881</td>\n",
              "      <td>0.079885</td>\n",
              "      <td>0.078807</td>\n",
              "      <td>-0.019219</td>\n",
              "      <td>0.191255</td>\n",
              "      <td>-0.058409</td>\n",
              "      <td>0.188844</td>\n",
              "      <td>0.059219</td>\n",
              "      <td>0.116842</td>\n",
              "      <td>0.030730</td>\n",
              "      <td>-0.021800</td>\n",
              "      <td>-0.133005</td>\n",
              "      <td>-0.149122</td>\n",
              "      <td>0.001810</td>\n",
              "      <td>-0.115539</td>\n",
              "      <td>0.275609</td>\n",
              "      <td>-0.062847</td>\n",
              "      <td>-0.063678</td>\n",
              "      <td>0.078652</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>-0.089402</td>\n",
              "      <td>-0.177425</td>\n",
              "      <td>0.162214</td>\n",
              "      <td>0.055362</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.140787</td>\n",
              "      <td>-0.177698</td>\n",
              "      <td>0.286300</td>\n",
              "      <td>-0.145609</td>\n",
              "      <td>-0.210482</td>\n",
              "      <td>-0.008774</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>-0.273529</td>\n",
              "      <td>-0.194581</td>\n",
              "      <td>-0.068206</td>\n",
              "      <td>0.166444</td>\n",
              "      <td>0.301647</td>\n",
              "      <td>-0.066956</td>\n",
              "      <td>-0.032055</td>\n",
              "      <td>0.318497</td>\n",
              "      <td>0.338836</td>\n",
              "      <td>-0.103396</td>\n",
              "      <td>0.187135</td>\n",
              "      <td>0.080959</td>\n",
              "      <td>-0.168924</td>\n",
              "      <td>-0.057638</td>\n",
              "      <td>-0.047540</td>\n",
              "      <td>0.120142</td>\n",
              "      <td>-5.613875</td>\n",
              "      <td>-0.430644</td>\n",
              "      <td>0.045277</td>\n",
              "      <td>-0.358954</td>\n",
              "      <td>-0.309674</td>\n",
              "      <td>-0.126182</td>\n",
              "      <td>-0.090772</td>\n",
              "      <td>-0.171695</td>\n",
              "      <td>-0.093239</td>\n",
              "      <td>-0.018116</td>\n",
              "      <td>0.257494</td>\n",
              "      <td>-0.042158</td>\n",
              "      <td>-0.187427</td>\n",
              "      <td>0.055787</td>\n",
              "      <td>0.397127</td>\n",
              "      <td>0.408273</td>\n",
              "      <td>-1.535803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.024408</td>\n",
              "      <td>0.034027</td>\n",
              "      <td>-0.127192</td>\n",
              "      <td>-0.237681</td>\n",
              "      <td>-0.221727</td>\n",
              "      <td>-0.177681</td>\n",
              "      <td>0.396425</td>\n",
              "      <td>0.460943</td>\n",
              "      <td>-0.206418</td>\n",
              "      <td>-0.334306</td>\n",
              "      <td>-0.070693</td>\n",
              "      <td>-0.120225</td>\n",
              "      <td>-0.340857</td>\n",
              "      <td>0.333172</td>\n",
              "      <td>0.205235</td>\n",
              "      <td>0.295793</td>\n",
              "      <td>-0.040484</td>\n",
              "      <td>0.202661</td>\n",
              "      <td>0.007596</td>\n",
              "      <td>0.110851</td>\n",
              "      <td>0.041424</td>\n",
              "      <td>0.100439</td>\n",
              "      <td>0.172731</td>\n",
              "      <td>0.084441</td>\n",
              "      <td>0.105909</td>\n",
              "      <td>0.006823</td>\n",
              "      <td>-0.008152</td>\n",
              "      <td>0.006714</td>\n",
              "      <td>-0.069525</td>\n",
              "      <td>0.052692</td>\n",
              "      <td>0.017281</td>\n",
              "      <td>0.023980</td>\n",
              "      <td>0.057797</td>\n",
              "      <td>-0.078362</td>\n",
              "      <td>0.173645</td>\n",
              "      <td>-0.105473</td>\n",
              "      <td>-0.111007</td>\n",
              "      <td>-0.191416</td>\n",
              "      <td>0.079825</td>\n",
              "      <td>0.058221</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059365</td>\n",
              "      <td>-0.121353</td>\n",
              "      <td>0.403970</td>\n",
              "      <td>-0.286452</td>\n",
              "      <td>-0.235863</td>\n",
              "      <td>0.146488</td>\n",
              "      <td>-0.161996</td>\n",
              "      <td>-0.261996</td>\n",
              "      <td>-0.261410</td>\n",
              "      <td>-0.143964</td>\n",
              "      <td>0.059866</td>\n",
              "      <td>0.276187</td>\n",
              "      <td>-0.017320</td>\n",
              "      <td>-0.076717</td>\n",
              "      <td>0.156914</td>\n",
              "      <td>0.427804</td>\n",
              "      <td>-0.069449</td>\n",
              "      <td>0.168566</td>\n",
              "      <td>-0.130331</td>\n",
              "      <td>-0.168458</td>\n",
              "      <td>0.152836</td>\n",
              "      <td>-0.063720</td>\n",
              "      <td>0.073317</td>\n",
              "      <td>-6.399953</td>\n",
              "      <td>-0.188364</td>\n",
              "      <td>0.043894</td>\n",
              "      <td>-0.209921</td>\n",
              "      <td>-0.076292</td>\n",
              "      <td>-0.131278</td>\n",
              "      <td>-0.049601</td>\n",
              "      <td>-0.088863</td>\n",
              "      <td>0.072378</td>\n",
              "      <td>-0.072859</td>\n",
              "      <td>0.348487</td>\n",
              "      <td>0.105427</td>\n",
              "      <td>-0.106488</td>\n",
              "      <td>0.093005</td>\n",
              "      <td>0.469043</td>\n",
              "      <td>0.384939</td>\n",
              "      <td>-2.307310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.056508</td>\n",
              "      <td>-0.016490</td>\n",
              "      <td>0.070608</td>\n",
              "      <td>-0.237288</td>\n",
              "      <td>-0.278764</td>\n",
              "      <td>-0.113179</td>\n",
              "      <td>0.587894</td>\n",
              "      <td>0.577017</td>\n",
              "      <td>-0.195557</td>\n",
              "      <td>-0.344989</td>\n",
              "      <td>-0.182985</td>\n",
              "      <td>-0.155044</td>\n",
              "      <td>-0.343415</td>\n",
              "      <td>0.373632</td>\n",
              "      <td>0.280687</td>\n",
              "      <td>0.203506</td>\n",
              "      <td>-0.114142</td>\n",
              "      <td>0.365524</td>\n",
              "      <td>0.008763</td>\n",
              "      <td>0.008182</td>\n",
              "      <td>0.190651</td>\n",
              "      <td>-0.165821</td>\n",
              "      <td>0.201333</td>\n",
              "      <td>-0.047057</td>\n",
              "      <td>0.308513</td>\n",
              "      <td>-0.026791</td>\n",
              "      <td>0.047471</td>\n",
              "      <td>-0.208322</td>\n",
              "      <td>-0.128534</td>\n",
              "      <td>-0.082064</td>\n",
              "      <td>-0.096253</td>\n",
              "      <td>0.230602</td>\n",
              "      <td>0.026898</td>\n",
              "      <td>0.052653</td>\n",
              "      <td>-0.123177</td>\n",
              "      <td>-0.191362</td>\n",
              "      <td>-0.220581</td>\n",
              "      <td>-0.164509</td>\n",
              "      <td>0.189017</td>\n",
              "      <td>0.303570</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144979</td>\n",
              "      <td>-0.210306</td>\n",
              "      <td>0.329226</td>\n",
              "      <td>-0.317236</td>\n",
              "      <td>-0.105944</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>0.078933</td>\n",
              "      <td>-0.264810</td>\n",
              "      <td>-0.192180</td>\n",
              "      <td>-0.188368</td>\n",
              "      <td>0.108339</td>\n",
              "      <td>0.339139</td>\n",
              "      <td>0.036471</td>\n",
              "      <td>-0.065010</td>\n",
              "      <td>0.319726</td>\n",
              "      <td>0.341164</td>\n",
              "      <td>0.126070</td>\n",
              "      <td>0.221267</td>\n",
              "      <td>0.152394</td>\n",
              "      <td>-0.091573</td>\n",
              "      <td>-0.064316</td>\n",
              "      <td>-0.099108</td>\n",
              "      <td>-0.038908</td>\n",
              "      <td>-5.804174</td>\n",
              "      <td>-0.287098</td>\n",
              "      <td>-0.034441</td>\n",
              "      <td>-0.143288</td>\n",
              "      <td>-0.453980</td>\n",
              "      <td>-0.358007</td>\n",
              "      <td>0.161565</td>\n",
              "      <td>-0.024294</td>\n",
              "      <td>-0.100231</td>\n",
              "      <td>-0.232839</td>\n",
              "      <td>0.170220</td>\n",
              "      <td>0.041876</td>\n",
              "      <td>0.039278</td>\n",
              "      <td>-0.149954</td>\n",
              "      <td>0.312199</td>\n",
              "      <td>0.522858</td>\n",
              "      <td>-1.472940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...       766       767  sentiment_score\n",
              "0 -0.087593 -0.014889 -0.047897  ...  0.398687  0.502432        -1.371694\n",
              "1  0.086113 -0.033744 -0.082821  ...  0.375111  0.354356        -1.236154\n",
              "2 -0.112473  0.044041 -0.021386  ...  0.397127  0.408273        -1.535803\n",
              "3  0.024408  0.034027 -0.127192  ...  0.469043  0.384939        -2.307310\n",
              "4 -0.056508 -0.016490  0.070608  ...  0.312199  0.522858        -1.472940\n",
              "\n",
              "[5 rows x 769 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEiWSJj9Ad_g",
        "colab_type": "text"
      },
      "source": [
        "Evaluating  Logistic Regression Classifier on the features obtained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc6tW4NFDxBE",
        "colab_type": "code",
        "outputId": "581633b9-6960-4d99-f54f-4cc8fcd735ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features_df, labels)\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=5000)\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "score = lr_clf.score(test_features, test_labels)\n",
        "print(\"Accuracy without scaling is: \",score)\n",
        "from sklearn import preprocessing\n",
        "# Scale\n",
        "scaled_train_features = preprocessing.scale(train_features)\n",
        "scaled_test_features = preprocessing.scale(test_features)\n",
        "\n",
        "lr_clf_scaled = LogisticRegression(max_iter=5000)\n",
        "lr_clf_scaled.fit(scaled_train_features, train_labels)\n",
        "score = lr_clf_scaled.score(scaled_test_features, test_labels)\n",
        "print(\"Accuracy with scaling is: \",score)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy without scaling is:  0.9282700421940928\n",
            "Accuracy with scaling is:  0.9240506329113924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUyYrzlkNR4I",
        "colab_type": "text"
      },
      "source": [
        "As we can see from above scaling the features barely affects the accuracy and hence we drop scaling moving forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkPSZ-JaArNj",
        "colab_type": "text"
      },
      "source": [
        "Evaluating Logistic Regression Classifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOgZYkAQ10RA",
        "colab_type": "code",
        "outputId": "c2e5db18-0762-40ad-cc59-92c8ecee7318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def unbiased_test_score(test_set,lr_clf_sentiment,model=lr_clf,return_mode=False):\n",
        "  test = vectorizeBERT(test_set,bert_type=\"distil\")\n",
        "  test_l = test_set[\"label\"]\n",
        "  test_score = lr_clf_sentiment.predict_log_proba(test)[:,0]\n",
        "  test_df = pd.DataFrame(test)\n",
        "  test_df[\"sentiment_score\"] = test_score\n",
        "  print(\"Score for Random unbiased Dataset\",model.score(test_df, test_l))\n",
        "  if return_mode:\n",
        "    return model.predict(test_df),model.predict_proba(test_df)[:,0],test_l\n",
        "\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,lr_clf)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1LVSgyPQcHV",
        "colab_type": "text"
      },
      "source": [
        "Evaluating Linear SVM on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PucNd_ctD_pt",
        "colab_type": "code",
        "outputId": "c093744b-6288-44ca-bc09-470e9b285c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "our_svm_model = SVC(kernel = \"linear\",C=1000,random_state=1)\n",
        "our_svm_model.fit(train_features, train_labels)\n",
        "biased_Score = our_svm_model.score(test_features, test_labels)\n",
        "print(\"Biased score for SVM is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,our_svm_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for SVM is:  0.9113924050632911\n",
            "Score for Random unbiased Dataset 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cA76JaEps6C",
        "colab_type": "text"
      },
      "source": [
        "Evaluating RandomForestClassifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwchE8yCpsO9",
        "colab_type": "code",
        "outputId": "f6f63bb4-809b-4417-cbc3-a6d0abbb734b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "modelRFC = RandomForestClassifier()\n",
        "modelRFC.fit(train_features, train_labels)\n",
        "biased_Score = modelRFC.score(test_features, test_labels)\n",
        "print(\"Biased score for RandomForestClassifier is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,modelRFC)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for RandomForestClassifier is:  0.9282700421940928\n",
            "Score for Random unbiased Dataset 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atbu5dAYQhgw",
        "colab_type": "text"
      },
      "source": [
        "Evaluating XGBoostClassifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvgDId-NqIq",
        "colab_type": "code",
        "outputId": "3fb1f204-5c70-4c51-f875-41ca74a2779c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "modelXGB = XGBClassifier()\n",
        "modelXGB.fit(train_features, train_labels)\n",
        "biased_Score = modelXGB.score(test_features, test_labels)\n",
        "print(\"Biased score for XGBoostClassifier is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,modelXGB)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for XGBoostClassifier is:  0.8945147679324894\n",
            "Score for Random unbiased Dataset 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvJVY9nOBbiQ",
        "colab_type": "code",
        "outputId": "06f53610-e176-4d0b-87ca-2fb2045d53f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
        "\n",
        "# Create param grid.\n",
        "\n",
        "param_grid = [\n",
        "    {'classifier' : [LogisticRegression(max_iter=5000)],\n",
        "     'classifier__penalty' : ['l1', 'l2'],\n",
        "    'classifier__C' : np.logspace(-4, 4, 20),\n",
        "    'classifier__solver' : ['liblinear','lbfgs']},\n",
        "    {'classifier' : [RandomForestClassifier()],\n",
        "    'classifier__n_estimators' : list(range(10,101,10)),\n",
        "    'classifier__max_features' : list(range(6,32,5))},\n",
        "    {'classifier' : [SVC(probability=True)],\n",
        "    'classifier__C' : np.logspace(-4, 4, 20),\n",
        "    'classifier__kernel' : ['linear','rbf']},\n",
        "    {'classifier' : [XGBClassifier()]}\n",
        "]\n",
        "\n",
        "# Create grid search object\n",
        "\n",
        "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "\n",
        "\n",
        "best_clf = clf.fit(train_features, train_labels)\n",
        "\n",
        "\n",
        "print('Best parameters are: ',best_clf.best_params_)\n",
        "print('Best score is: ',best_clf.best_score_)\n",
        "\n",
        "pd.DataFrame(best_clf.cv_results_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 181 candidates, totalling 905 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.8s\n",
            "[Parallel(n_jobs=-1)]: Done 380 tasks      | elapsed:   46.7s\n",
            "[Parallel(n_jobs=-1)]: Done 630 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 905 out of 905 | elapsed:  5.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters are:  {'classifier': SVC(C=78.47599703514607, break_ties=False, cache_size=200, class_weight=None,\n",
            "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n",
            "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
            "    shrinking=True, tol=0.001, verbose=False), 'classifier__C': 78.47599703514607, 'classifier__kernel': 'rbf'}\n",
            "Best score is:  0.9114251945237861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_classifier</th>\n",
              "      <th>param_classifier__C</th>\n",
              "      <th>param_classifier__penalty</th>\n",
              "      <th>param_classifier__solver</th>\n",
              "      <th>param_classifier__max_features</th>\n",
              "      <th>param_classifier__n_estimators</th>\n",
              "      <th>param_classifier__kernel</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.044566</td>\n",
              "      <td>0.011850</td>\n",
              "      <td>0.004359</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l1</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.524476</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.526022</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010837</td>\n",
              "      <td>0.006349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l1</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.039081</td>\n",
              "      <td>0.006617</td>\n",
              "      <td>0.004213</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l2</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.524476</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.526022</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.032159</td>\n",
              "      <td>0.004981</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l2</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.524476</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.526022</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.035619</td>\n",
              "      <td>0.004895</td>\n",
              "      <td>0.006680</td>\n",
              "      <td>0.005074</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.000263665</td>\n",
              "      <td>l1</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.524476</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.528169</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.526022</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>1.047201</td>\n",
              "      <td>0.061486</td>\n",
              "      <td>0.039791</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>SVC(C=78.47599703514607, break_ties=False, cac...</td>\n",
              "      <td>3792.69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>linear</td>\n",
              "      <td>{'classifier': SVC(C=78.47599703514607, break_...</td>\n",
              "      <td>0.874126</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.845070</td>\n",
              "      <td>0.915493</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.879051</td>\n",
              "      <td>0.022405</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>1.121406</td>\n",
              "      <td>0.062388</td>\n",
              "      <td>0.047739</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>SVC(C=78.47599703514607, break_ties=False, cac...</td>\n",
              "      <td>3792.69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'classifier': SVC(C=78.47599703514607, break_...</td>\n",
              "      <td>0.881119</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.922535</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.898759</td>\n",
              "      <td>0.018523</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>1.032049</td>\n",
              "      <td>0.053672</td>\n",
              "      <td>0.041530</td>\n",
              "      <td>0.004105</td>\n",
              "      <td>SVC(C=78.47599703514607, break_ties=False, cac...</td>\n",
              "      <td>10000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>linear</td>\n",
              "      <td>{'classifier': SVC(C=78.47599703514607, break_...</td>\n",
              "      <td>0.874126</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.845070</td>\n",
              "      <td>0.915493</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.879051</td>\n",
              "      <td>0.022405</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1.157407</td>\n",
              "      <td>0.109814</td>\n",
              "      <td>0.047534</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>SVC(C=78.47599703514607, break_ties=False, cac...</td>\n",
              "      <td>10000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'classifier': SVC(C=78.47599703514607, break_...</td>\n",
              "      <td>0.881119</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.922535</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.898759</td>\n",
              "      <td>0.018523</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>4.510883</td>\n",
              "      <td>0.629344</td>\n",
              "      <td>0.020494</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': XGBClassifier(base_score=0.5, b...</td>\n",
              "      <td>0.832168</td>\n",
              "      <td>0.859155</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.901408</td>\n",
              "      <td>0.857983</td>\n",
              "      <td>0.030785</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0         0.044566      0.011850  ...        0.002835              145\n",
              "1         0.010837      0.006349  ...             NaN              163\n",
              "2         0.039081      0.006617  ...        0.002835              145\n",
              "3         0.032159      0.004981  ...        0.002835              145\n",
              "4         0.035619      0.004895  ...        0.002835              145\n",
              "..             ...           ...  ...             ...              ...\n",
              "176       1.047201      0.061486  ...        0.022405               31\n",
              "177       1.121406      0.062388  ...        0.018523                2\n",
              "178       1.032049      0.053672  ...        0.022405               31\n",
              "179       1.157407      0.109814  ...        0.018523                2\n",
              "180       4.510883      0.629344  ...        0.030785               69\n",
              "\n",
              "[181 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhCcK_7UC-a",
        "colab_type": "code",
        "outputId": "be37b04e-7e7d-41e4-8f9c-ddadfe189fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unbiased_test_score(test_set,lr_clf_sentiment,best_clf)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Btju4uR1Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluation_parameters(y_test,y_pred):\n",
        "  con_mat = confusion_matrix(y_test,y_pred)\n",
        "  tn, fp, fn, tp = con_mat.ravel()\n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  f1 = 2*precision*recall/(precision+recall)\n",
        "  acc = (tp+tn)/(tn+fn+fp+tp)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(con_mat)\n",
        "  print(\"The precision is: \",precision)\n",
        "  print(\"The recall is: \",recall)\n",
        "  print(\"The f1_score is: \",f1)\n",
        "  print(\"The accuracy is: \",acc)\n",
        "\n",
        "def plot_roc(y_test,y_score):\n",
        "  fpr,tpr,thresh=roc_curve(y_test,y_score)\n",
        "  plt.plot(fpr,tpr)\n",
        "  plt.grid()\n",
        "  plt.ylim(0,1.1)\n",
        "  print(\"The roc score is \",roc_auc_score(y_test,y_score))\n",
        "  plt.xlabel(\"Specificity\")\n",
        "  plt.ylabel(\"Sensitivity\")\n",
        "  plt.title(\"ROC Curve\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeignRGpSHHk",
        "colab_type": "code",
        "outputId": "fee3e70d-1ecd-405f-e50f-e919cb3f6100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred,y_score,y_test = unbiased_test_score(test_set,lr_clf_sentiment,best_clf,True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ET29ddLYFUi",
        "colab_type": "code",
        "outputId": "f222fe12-7903-43d1-db58-99b8e1bc4ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "evaluation_parameters(y_test,y_pred)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[18  6]\n",
            " [ 3 23]]\n",
            "The precision is:  0.7931034482758621\n",
            "The recall is:  0.8846153846153846\n",
            "The f1_score is:  0.8363636363636363\n",
            "The accuracy is:  0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qerybn2JYKtp",
        "colab_type": "code",
        "outputId": "d3977a51-e40e-4a3e-f294-074de99146dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plot_roc(y_test,y_score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The roc score is  0.12339743589743588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAZAUlEQVR4nO3de5RmVXnn8e9PEG/QQmzTNpfYqKDpaERTAq0mFhEV0UASHQXGURy0YxKCgnENyowi0RWjES8R0fYS0MhNMdJOQLyWeAEEhovQimkBoZEZRGmgkYDiM3+8p8NLUdX1VnWdt7rqfD9r1apz2eecZ1d1v0/tffbZJ1WFJKm7HjTXAUiS5paJQJI6zkQgSR1nIpCkjjMRSFLHmQgkqeNMBJLUcSYCLShJrktyV5INSf5vkpOSbDuuzDOTfD3JHUluS/LFJMvHlVmU5P1Jrm/O9eNmffEk102SI5JcmeTOJOuSfDbJU9qsrzQbTARaiP6kqrYF9gCeBrx5444kK4AvA2cBOwK7ApcD30nyuKbMNsDXgN8D9gMWASuAnwN7TnLNDwCvB44AfgvYHfgC8KLpBp9k6+keI22O+GSxFpIk1wGvqaqvNuvvBn6vql7UrH8L+H5V/dW4484BflZVr0zyGuCdwOOrasMA19wN+CGwoqq+N0mZMeBfqurjzfqhTZzPbtYLOBx4A7A18CXgzqr6275znAV8s6qOT7Ij8E/AHwEbgPdV1QcH+BFJD2CLQAtWkp2BFwJrm/WHA88EPjtB8TOA5zXL+wJfGiQJNJ4LrJssCUzDnwJ7AcuBU4GXJwlAkh2A5wOnJXkQ8EV6LZmdmuu/IckLNvP66igTgRaiLyS5A7gBuBl4W7P9t+j9m79pgmNuAjb2/z9qkjKTmW75yfx9Vf2iqu4CvgUU8IfNvpcC51fVT4FnAI+uquOq6p6qugb4GHDQLMSgDjIRaCH606raDhgFnsR9H/C3Ar8Blk5wzFLglmb555OUmcx0y0/mho0L1euzPQ04uNl0CPCZZvmxwI5J1m/8At4CLJmFGNRBJgItWFX1TeAk4B+b9TuB84H/MkHxl9G7QQzwVeAFSR4x4KW+BuycZGQTZe4EHt63/piJQh63firw0iSPpddldGaz/Qbg2qravu9ru6raf8B4pfsxEWihez/wvCRPbdaPBl7VDPXcLskOSd5Bb1TQ25syn6b3YXtmkicleVCSRyV5S5IHfNhW1b8DHwZOTTKaZJskD01yUJKjm2KXAX+e5OFJngAcNlXgVXUpvVbKx4Fzq2p9s+t7wB1J/keShyXZKsmTkzxjJj8gyUSgBa2qfgZ8Cnhrs/5t4AXAn9Pr1/8JvSGmz24+0Kmqu+ndMP4h8BXgdnofvouBCye51BHAh4ATgPXAj4E/o3dTF+B9wD3A/wNO5r5unqmc0sRySl+d7gVeTG947LXclyweOeA5pftx+KgkdZwtAknqOBOBJHWciUCSOs5EIEkdN+8mt1q8eHEtW7ZsRsfeeeedPOIRgw4NXxisczdY527YnDpfcsklt1TVoyfaN+8SwbJly7j44otndOzY2Bijo6OzG9AWzjp3g3Xuhs2pc5KfTLbPriFJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUseZCCSp40wEktRxJgJJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUseZCCSp41pLBEk+meTmJFdOsj9JPphkbZIrkjy9rVgkSZNrs0VwErDfJva/ENit+VoJnNhiLJKkSbSWCKrqPOAXmyhyIPCp6rkA2D7J0rbikaT57O1fvIrP/ODuVs49l6+q3Am4oW99XbPtpvEFk6yk12pgyZIljI2NzeiCGzZsmPGx85V17gbrvPB9d81d3Hvvva3UeV68s7iqVgGrAEZGRmqm7+z0HafdYJ27oWt1PvHq81m/fn0rdZ7LUUM3Arv0re/cbJMkDdFcJoLVwCub0UN7A7dV1QO6hSRJ7WqtayjJqcAosDjJOuBtwIMBquojwNnA/sBa4JfAq9uKRZI0udYSQVUdPMX+Av66retLkgbjk8WS1HEmAknqOBOBJHWciUCSOs5EIEkdZyKQpI4zEUhSx5kIJKnj5sWkc5K0kJxy4fWcddn0plZbc9Pt7PiwduKxRSBJQ3bWZTey5qbbp3XM8qWLWLFjO3+72yKQpDmwfOkiTv+LFdM6pq33L9gikKSOMxFIUseZCCSp40wEktRxJgJJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUseZCCSp40wEktRxJgJJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUseZCCSp40wEktRxrSaCJPsluTrJ2iRHT7D/d5J8I8mlSa5Isn+b8UiSHqi1RJBkK+AE4IXAcuDgJMvHFfufwBlV9TTgIODDbcUjSZpYmy2CPYG1VXVNVd0DnAYcOK5MAYua5UcCP20xHknSBFJV7Zw4eSmwX1W9pln/b8BeVXV4X5mlwJeBHYBHAPtW1SUTnGslsBJgyZIlf3DaaafNKKYNGzaw7bbbzujY+co6d4N1nl/+/sK7AHjzXg+b1nGbU+d99tnnkqoamWjf1jM64+w5GDipqt6bZAXw6SRPrqrf9BeqqlXAKoCRkZEaHR2d0cXGxsaY6bHzlXXuBus8v5x49fkAjI6umNZxbdW5za6hG4Fd+tZ3brb1Oww4A6CqzgceCixuMSZJ0jhtJoKLgN2S7JpkG3o3g1ePK3M98FyAJL9LLxH8rMWYJEnjtJYIqurXwOHAucAP6I0OuirJcUkOaIq9EXhtksuBU4FDq62bFpKkCbV6j6CqzgbOHrftrX3La4BntRmDJGnTfLJYkjrORCBJHWcikKSOMxFIUseZCCSp40wEktRxJgJJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUseZCCSp40wEktRxJgJJ6jgTgSR1nIlAkjrORCBJHWcikKSOMxFIUscNlAiSfD7Ji5KYOCRpgRn0g/3DwCHAvyd5V5InthiTJGmIBkoEVfXVqvqvwNOB64CvJvluklcneXCbAUqS2jVwV0+SRwGHAq8BLgU+QC8xfKWVyCRJQ7H1IIWS/CvwRODTwJ9U1U3NrtOTXNxWcJKk9g2UCICPVdXZ/RuSPKSq7q6qkRbikiQNyaBdQ++YYNv5sxmIJGlubLJFkOQxwE7Aw5I8DUizaxHw8JZjkyQNwVRdQy+gd4N4Z+D4vu13AG9pKSZJ0hBtMhFU1cnAyUleUlVnDikmSdIQTdU19Iqq+hdgWZKjxu+vquMnOKz/+P3oDTPdCvh4Vb1rgjIvA44FCri8qg4ZPHxJ0uaaqmvoEc33bad74iRbAScAzwPWARclWV1Va/rK7Aa8GXhWVd2a5Lenex1J0uaZqmvoo83ih6vqZ9M8957A2qq6BiDJacCBwJq+Mq8FTqiqW5vr3TzNa0iSNtOgzxF8J8l1wOnA5zd+cE9hJ+CGvvV1wF7jyuwOkOQ79LqPjq2qL40/UZKVwEqAJUuWMDY2NmDY97dhw4YZHztfWedusM7zy/r1dwFMO/626jxQIqiq3ZPsCRwEHJNkDXBac/9gc6+/GzBKb2TSeUmeUlXrx11/FbAKYGRkpEZHR2d0sbGxMWZ67HxlnbvBOs8vJ17dewxrdHTFtI5rq84DzzVUVd+rqqPodfn8Ajh5ikNuBHbpW9+52dZvHbC6qn5VVdcCP6KXGCRJQzLo+wgWJXlVknOA7wI30UsIm3IRsFuSXZNsQ681sXpcmS/Qaw2QZDG9rqJrBg9fkrS5Br1HcDm9D+3jqmqgqSWq6tdJDgfOpdf//8mquirJccDFVbW62ff8pqvpXuBNVfXzaddCkjRjgyaCx1VVTffkzUR1Z4/b9ta+5QKOar4kSXNgqgfK3l9VbwBWJ3lAIqiqA1qLTJI0FFO1CD7dfP/HtgORJM2NqR4ou6RZ3KOqPtC/L8nrgW+2FZgkaTgGHT76qgm2HTqLcUiS5shU9wgOBg4Bdk3SP/RzO3rPEkiS5rmp7hFsfGZgMfDevu13AFe0FZQkaXimukfwE+AnwPSeg5YkzRtTdQ19u6qeneQOeu8L+M9d9B4DWNRqdJKk1k3VInh283274YQjSRq2QecaenyShzTLo0mOSLJ9u6FJkoZh0OGjZwL3JnkCvemgdwFOaS0qSdLQDJoIflNVvwb+DPinqnoTsLS9sCRJwzJoIvhV80zBq4D/3Wx7cDshSZKGadBE8Gp6Q0jfWVXXJtmV++YhkiTNY4O+qnINcETf+rXAP7QVlCRpeAZKBEmeBRwLPLY5ZuNzBI9rLzRJ0jAM+mKaTwBHApfQe5OYJGmBGDQR3FZV57QaiSRpTgyaCL6R5D3A54G7N26sqv/TSlSSpKEZNBHs1Xwf6dtWwB/PbjiSpGEbdNTQPm0HIkmaG4PONbQkySeSnNOsL09yWLuhSZKGYdAHyk4CzgV2bNZ/BLyhjYAkScM1aCJYXFVnAL8BaOYdchipJC0AgyaCO5M8iublNEn2Bm5rLSpJ0tAMOmroKGA18Pgk3wEeDby0tagkSUOzyRZBkmckeUzzvMBzgLfQe47gy8C6IcQnSWrZVF1DHwXuaZafCRwDnADcSu8FNZKkeW6qrqGtquoXzfLLgVVVdSZwZpLL2g1NkjQMU7UItkqyMVk8F/h6375B7y9IkrZgU32Ynwp8M8ktwF3AtwCadxc7akiSFoBNtgiq6p3AG+k9UPbsqqq+4/5mqpMn2S/J1UnWJjl6E+VekqSSjExWRpLUjim7d6rqggm2/Wiq45JsRe/G8vPojTC6KMnq5m1n/eW2A14PXDho0JKk2TPoA2UzsSewtqquqap7gNOAAyco93f0Xnv5Hy3GIkmaRJs3fHcCbuhbX8d901kDkOTpwC5V9W9J3jTZiZKsBFYCLFmyhLGxsRkFtGHDhhkfO19Z526wzvPL+vV3AUw7/rbqPGcjf5I8CDgeOHSqslW1iua5hZGRkRodHZ3RNcfGxpjpsfOVde4G6zy/nHj1+QCMjq6Y1nFt1bnNrqEbgV361ndutm20HfBkYCzJdcDewGpvGEvScLWZCC4Cdkuya5JtgIPozVcEQFXdVlWLq2pZVS0DLgAOqKqLW4xJkjROa4mgmar6cHrvMfgBcEZVXZXkuCQHtHVdSdL0tHqPoKrOBs4et+2tk5QdbTMWSdLE2uwakiTNAyYCSeo4E4EkdZyJQJI6zkQgSR1nIpCkjjMRSFLHmQgkqeNMBJLUcSYCSeo4E4EkdZyJQJI6zkQgSR1nIpCkjjMRSFLHmQgkqeNMBJLUcSYCSeo4E4EkdVyr7yyWpIXulAuv56zLbpzWMWtuup3lSxe1FNH02SKQpM1w1mU3suam26d1zPKlizhwj51aimj6bBFI0mZavnQRp//FirkOY8ZsEUhSx5kIJKnjTASS1HEmAknqOG8WS1Kf6Q4H3dKGgs6ELQJJ6jPd4aBb2lDQmbBFIEnjzPfhoNNli0CSOq7VRJBkvyRXJ1mb5OgJ9h+VZE2SK5J8Lclj24xHkvRArSWCJFsBJwAvBJYDBydZPq7YpcBIVf0+8Dng3W3FI0maWJstgj2BtVV1TVXdA5wGHNhfoKq+UVW/bFYvAHZuMR5J0gTavFm8E3BD3/o6YK9NlD8MOGeiHUlWAisBlixZwtjY2IwC2rBhw4yPna+sczdY59mzfv1dAFvkz7OtOm8Ro4aSvAIYAZ4z0f6qWgWsAhgZGanR0dEZXWdsbIyZHjtfWedu6EKdx4/vX7/+Lrbf/iGzfp2f3nU3y5cuYnR0yxs11Nbvuc2uoRuBXfrWd2623U+SfYFjgAOq6u4W45E0j81kuueZWAjPBUxXmy2Ci4DdkuxKLwEcBBzSXyDJ04CPAvtV1c0txiJpAegf39/763jL+6t9PmqtRVBVvwYOB84FfgCcUVVXJTkuyQFNsfcA2wKfTXJZktVtxSNJmlir9wiq6mzg7HHb3tq3vG+b15ckTc0niyWp40wEktRxW8TwUUnd08XpnrdUtggkzYkuTve8pbJFIGnOdG265y2VLQJJ6jgTgSR1nIlAkjrORCBJHWcikKSOc9SQtMBNd7z+sPhcwJbDFoG0wA1r+ubp8rmALYctAqkDHK+vTbFFIEkdZyKQpI4zEUhSx5kIJKnjTASS1HGOGpJmwVyP1V+//i5OvPr8Cfc5Xl9TsUUgzYItdaw+OF5fU7NFIM2SuRyrPzY2xuiozwloZmwRSFLHmQgkqeNMBJLUcSYCSeo4bxZvwlwPCZwNmxpWuFDNRZ0doqn5zBbBJmzJQwK1ZXGIpuYzWwRTmO/T93ZxWGEX6yxtDlsEktRxJgJJ6jgTgSR1XKuJIMl+Sa5OsjbJ0RPsf0iS05v9FyZZ1mY8kqQHai0RJNkKOAF4IbAcODjJ8nHFDgNuraonAO8D/qGteCRJE2tz1NCewNqqugYgyWnAgcCavjIHAsc2y58DPpQkVVWzHczbv3gV310zvfHljg2X1AVtJoKdgBv61tcBe01Wpqp+neQ24FHALf2FkqwEVjarG5JcPcOYFo8/91SuBM543QyvtmWYdp0XAOvcDdZ5eh472Y558RxBVa0CVm3ueZJcXFUjsxDSvGGdu8E6d0NbdW7zZvGNwC596zs32yYsk2Rr4JHAz1uMSZI0TpuJ4CJgtyS7JtkGOAhYPa7MauBVzfJLga+3cX9AkjS51rqGmj7/w4Fzga2AT1bVVUmOAy6uqtXAJ4BPJ1kL/IJesmjTZncvzUPWuRuscze0Uuf4B7gkdZtPFktSx5kIJKnjFmQi6OLUFgPU+agka5JckeRrSSYdUzxfTFXnvnIvSVJJ5v1Qw0HqnORlze/6qiSnDDvG2TbAv+3fSfKNJJc2/773n4s4Z0uSTya5OcmVk+xPkg82P48rkjx9sy9aVQvqi96N6R8DjwO2AS4Hlo8r81fAR5rlg4DT5zruIdR5H+DhzfJfdqHOTbntgPOAC4CRuY57CL/n3YBLgR2a9d+e67iHUOdVwF82y8uB6+Y67s2s8x8BTweunGT//sA5QIC9gQs395oLsUXwn1NbVNU9wMapLfodCJzcLH8OeG6SDDHG2TZlnavqG1X1y2b1AnrPdcxng/yeAf6O3hxW/zHM4FoySJ1fC5xQVbcCVNXNQ45xtg1S5wI2zgXzSOCnQ4xv1lXVefRGUU7mQOBT1XMBsH2SpZtzzYWYCCaa2mL8OwTvN7UFsHFqi/lqkDr3O4zeXxTz2ZR1bprMu1TVvw0zsBYN8nveHdg9yXeSXJBkv6FF145B6nws8Iok64Czgb8ZTmhzZrr/36c0L6aY0OxJ8gpgBHjOXMfSpiQPAo4HDp3jUIZta3rdQ6P0Wn3nJXlKVa2f06jadTBwUlW9N8kKes8mPbmqfjPXgc0XC7FF0MWpLQapM0n2BY4BDqiqu4cUW1umqvN2wJOBsSTX0etLXT3PbxgP8nteB6yuql9V1bXAj+glhvlqkDofBpwBUFXnAw+lNznbQjXQ//fpWIiJoItTW0xZ5yRPAz5KLwnM935jmKLOVXVbVS2uqmVVtYzefZEDquriuQl3Vgzyb/sL9FoDJFlMr6vommEGOcsGqfP1wHMBkvwuvUTws6FGOVyrgVc2o4f2Bm6rqps254QLrmuotsypLVo1YJ3fA2wLfLa5L359VR0wZ0FvpgHrvKAMWOdzgecnWQPcC7ypquZta3fAOr8R+FiSI+ndOD50Pv9hl+RUesl8cXPf423AgwGq6iP07oPsD6wFfgm8erOvOY9/XpKkWbAQu4YkSdNgIpCkjjMRSFLHmQgkqeNMBJLUcSYCdUaSY5oZOa9IclmSvWbx3Gcn2b5ZPiLJD5J8JskBm5oZtSn/3eb7siSHzFZM0qAcPqpOaKYeOB4Yraq7m4ettqmqWZ+gLMkPgX2rat00jxsF/raqXjzbMUmbYotAXbEUuGXj1BpVdUtV/TTJdUneneT7Sb6X5AkASR6d5MwkFzVfz2q2b5vkn5vyVyR5SbP9uiSLk3yE3pTJ5yQ5MsmhST7UlFmS5F+TXN58PbPZvqGJ8V3AHzatlSOTnJdkj40VSPLtJE8d0s9LHWIiUFd8GdglyY+SfDhJ/6R7t1XVU4APAe9vtn0AeF9VPQN4CfDxZvv/2li+qn4f+Hr/RarqdfSmQd6nqt43LoYPAt+sqqfSm2/+qnH7jwa+VVV7NMd+gmbSvCS7Aw+tqstnWH9pUiYCdUJVbQD+AFhJbx6a05Mc2uw+te/7imZ5X+BDSS6jN7fLoiTbNttP6DvvrdMI44+BE5vj7q2q26Yo/1ngxUkeDPx34KRpXEsa2IKba0iaTFXdC4zRm5H0+9w38WD/jbKNyw8C9q6q+73QZpjvL6qqXyb5Cr0XkbyMXiKTZp0tAnVCkicm6Z+OeQ/gJ83yy/u+n98sf5m+F5z09dV/Bfjrvu07TCOMr9F7TShJtkryyHH776A3fXa/j9PrUrpomq0PaWAmAnXFtsDJ6b3U/Qp677Y9ttm3Q7Pt9cCRzbYjgJHmhvAa4HXN9nc05a9Mcjm9d0EP6vXAPk1r5JImhn5XAPc2N5KPBKiqS4DbgX+exnWkaXH4qDqteWnNSFXdMtexTCTJjvS6s57kG7fUFlsE0hYqySuBC4FjTAJqky0CSeo4WwSS1HEmAknqOBOBJHWciUCSOs5EIEkd9/8BBm1ONzZ7CY8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIL_Dl624eVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(best_clf, open(\"best_clf_distilbert\", 'wb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}