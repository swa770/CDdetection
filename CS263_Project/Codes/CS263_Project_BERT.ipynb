{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS263_Project_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsWUcIFJ8CAp",
        "colab_type": "text"
      },
      "source": [
        "Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1A6tOerXST0",
        "colab_type": "code",
        "outputId": "76b2d037-94c5-41f1-fdfe-92a4754655a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/CS263_Project/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/CS263_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urxdiTR88JDy",
        "colab_type": "text"
      },
      "source": [
        "Loading all datasets from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcAEcWu6ZolC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(r\"all_data.csv\")\n",
        "test_set = pd.read_csv(r\"test_set.csv\")\n",
        "sentiment_data = pd.read_csv(r\"sentiment.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jVqda3S8ace",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a913795d-19a1-4c1a-abcf-10e4c0212d1e"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.92)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkG7aoqf8hbk",
        "colab_type": "text"
      },
      "source": [
        "Function for BERT Vectorization is defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFk5zLSv8fuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def vectorizeBERT(df, bert_type = \"bert\"):\n",
        "\n",
        "  if bert_type == \"distil\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "  elif bert_type == \"roberta\":\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.RobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
        "  else:\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "  # Load pretrained model/tokenizer\n",
        "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "  model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "  tokenized = df[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "  # padding step\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "  attention_mask.shape\n",
        "\n",
        "  input_ids = torch.tensor(np.array(padded))\n",
        "  attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "  # Slice the output for the first position for all the sequences, take all hidden unit outputs\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "\n",
        "  return features\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxA8eWS_Xd1",
        "colab_type": "text"
      },
      "source": [
        "Function for obtaining sentiment score using sentiment data is defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mof6wXn-z-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def obtain_sentiment_classifier(sentiment_data,data_select=4000):\n",
        "  import random\n",
        "  import pickle\n",
        "  #Randomly selecting a small portion of sentiment data, since the dataset is too big\n",
        "  rand_ind = random.sample(range(sentiment_data.shape[0]),data_select)\n",
        "  sentiment=sentiment_data.iloc[rand_ind]\n",
        "  # Build the sentiment model\n",
        "  features = vectorizeBERT(sentiment)\n",
        "  features_ds = pd.DataFrame(features)\n",
        "  labels = sentiment[\"label\"]\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features_ds, labels)\n",
        "  #Logistic Regression classifier for sentiment analysis\n",
        "  lr_clf_sentiment = LogisticRegression(max_iter=5000)\n",
        "  lr_clf_sentiment.fit(train_features, train_labels)\n",
        "  #Calculate score of logistic regression classifier on sentiment data\n",
        "  sent_accuracy = lr_clf_sentiment.score(test_features, test_labels)\n",
        "  print(\"The accuracy of Logistic Regression classifier on sentiment data is: \",sent_accuracy)\n",
        "  pickle.dump(lr_clf_sentiment, open(\"sentiment_model\", 'wb'))\n",
        "  return lr_clf_sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G93tK-j8xh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e01a6fc-e9d6-4912-fbf0-0f7eff29c30e"
      },
      "source": [
        "# Build the cognitive distortion model\n",
        "# M x N matrix\n",
        "import time\n",
        "start_time = time.time()\n",
        "features = vectorizeBERT(df)\n",
        "print(\"Time taken is:\",time.time()-start_time)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken is: 153.5158336162567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C47k6CTXAS64",
        "colab_type": "text"
      },
      "source": [
        "Creating training dataset with features and sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaB1h_A58yWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "aa589b00-9390-4206-81e3-838bf617357d"
      },
      "source": [
        "labels = df[\"label\"]\n",
        "features_df = pd.DataFrame(features)\n",
        "lr_clf_sentiment = obtain_sentiment_classifier(sentiment_data,data_select=4000)\n",
        "sentiment_score = lr_clf_sentiment.predict_log_proba(features)[:,0]\n",
        "features_df[\"sentiment_score\"] = sentiment_score\n",
        "features_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of Logistic Regression classifier on sentiment data is:  0.809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>sentiment_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.064185</td>\n",
              "      <td>0.116251</td>\n",
              "      <td>0.173751</td>\n",
              "      <td>-0.136374</td>\n",
              "      <td>-0.227259</td>\n",
              "      <td>-0.503813</td>\n",
              "      <td>0.664396</td>\n",
              "      <td>0.809819</td>\n",
              "      <td>-0.078599</td>\n",
              "      <td>-0.401530</td>\n",
              "      <td>-0.008818</td>\n",
              "      <td>-0.270047</td>\n",
              "      <td>-0.016984</td>\n",
              "      <td>0.657804</td>\n",
              "      <td>0.507452</td>\n",
              "      <td>-0.013487</td>\n",
              "      <td>-0.046433</td>\n",
              "      <td>0.574368</td>\n",
              "      <td>0.152785</td>\n",
              "      <td>-0.214532</td>\n",
              "      <td>0.050718</td>\n",
              "      <td>-0.262154</td>\n",
              "      <td>-0.131824</td>\n",
              "      <td>-0.226232</td>\n",
              "      <td>-0.285502</td>\n",
              "      <td>-0.124004</td>\n",
              "      <td>-0.278517</td>\n",
              "      <td>0.019781</td>\n",
              "      <td>0.174474</td>\n",
              "      <td>0.003650</td>\n",
              "      <td>-0.156326</td>\n",
              "      <td>-0.084894</td>\n",
              "      <td>-0.225917</td>\n",
              "      <td>0.195152</td>\n",
              "      <td>0.170384</td>\n",
              "      <td>-0.102753</td>\n",
              "      <td>-0.267985</td>\n",
              "      <td>-0.179697</td>\n",
              "      <td>0.301730</td>\n",
              "      <td>0.115410</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106473</td>\n",
              "      <td>-0.060724</td>\n",
              "      <td>0.394658</td>\n",
              "      <td>-0.320591</td>\n",
              "      <td>-0.124769</td>\n",
              "      <td>-0.002194</td>\n",
              "      <td>0.049406</td>\n",
              "      <td>-0.493999</td>\n",
              "      <td>-0.146199</td>\n",
              "      <td>-0.163771</td>\n",
              "      <td>0.394479</td>\n",
              "      <td>0.196617</td>\n",
              "      <td>-0.117926</td>\n",
              "      <td>0.008908</td>\n",
              "      <td>0.401485</td>\n",
              "      <td>0.522761</td>\n",
              "      <td>0.008527</td>\n",
              "      <td>-0.097613</td>\n",
              "      <td>0.070396</td>\n",
              "      <td>-0.090552</td>\n",
              "      <td>-0.031218</td>\n",
              "      <td>0.301031</td>\n",
              "      <td>0.295522</td>\n",
              "      <td>-7.521424</td>\n",
              "      <td>-0.152080</td>\n",
              "      <td>-0.155668</td>\n",
              "      <td>-0.248154</td>\n",
              "      <td>-0.139791</td>\n",
              "      <td>-0.608704</td>\n",
              "      <td>0.055255</td>\n",
              "      <td>-0.142869</td>\n",
              "      <td>-0.095100</td>\n",
              "      <td>-0.106877</td>\n",
              "      <td>0.115598</td>\n",
              "      <td>0.115215</td>\n",
              "      <td>-0.001469</td>\n",
              "      <td>-0.145779</td>\n",
              "      <td>0.339752</td>\n",
              "      <td>0.522551</td>\n",
              "      <td>-2.150020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.173912</td>\n",
              "      <td>0.023234</td>\n",
              "      <td>-0.282618</td>\n",
              "      <td>-0.165053</td>\n",
              "      <td>-0.424329</td>\n",
              "      <td>-0.311755</td>\n",
              "      <td>0.568792</td>\n",
              "      <td>0.456151</td>\n",
              "      <td>-0.200455</td>\n",
              "      <td>-0.262142</td>\n",
              "      <td>0.186468</td>\n",
              "      <td>-0.105735</td>\n",
              "      <td>-0.298163</td>\n",
              "      <td>0.657450</td>\n",
              "      <td>0.516508</td>\n",
              "      <td>-0.071780</td>\n",
              "      <td>0.254133</td>\n",
              "      <td>0.539748</td>\n",
              "      <td>0.280533</td>\n",
              "      <td>-0.106023</td>\n",
              "      <td>-0.048515</td>\n",
              "      <td>-0.139862</td>\n",
              "      <td>0.259962</td>\n",
              "      <td>0.153837</td>\n",
              "      <td>-0.333290</td>\n",
              "      <td>-0.192454</td>\n",
              "      <td>-0.161889</td>\n",
              "      <td>0.002372</td>\n",
              "      <td>0.055239</td>\n",
              "      <td>-0.009457</td>\n",
              "      <td>0.122488</td>\n",
              "      <td>0.009130</td>\n",
              "      <td>-0.309751</td>\n",
              "      <td>-0.100182</td>\n",
              "      <td>0.190423</td>\n",
              "      <td>0.069538</td>\n",
              "      <td>-0.310809</td>\n",
              "      <td>-0.221255</td>\n",
              "      <td>-0.004161</td>\n",
              "      <td>0.260224</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070100</td>\n",
              "      <td>-0.147870</td>\n",
              "      <td>0.492935</td>\n",
              "      <td>-0.420588</td>\n",
              "      <td>0.061425</td>\n",
              "      <td>0.136778</td>\n",
              "      <td>0.015730</td>\n",
              "      <td>-0.258102</td>\n",
              "      <td>-0.140906</td>\n",
              "      <td>-0.376467</td>\n",
              "      <td>0.230319</td>\n",
              "      <td>0.299862</td>\n",
              "      <td>-0.293175</td>\n",
              "      <td>0.233682</td>\n",
              "      <td>0.539116</td>\n",
              "      <td>0.373029</td>\n",
              "      <td>-0.073583</td>\n",
              "      <td>-0.037665</td>\n",
              "      <td>-0.203862</td>\n",
              "      <td>0.212516</td>\n",
              "      <td>0.110676</td>\n",
              "      <td>0.097686</td>\n",
              "      <td>0.256059</td>\n",
              "      <td>-8.026570</td>\n",
              "      <td>-0.096856</td>\n",
              "      <td>-0.250646</td>\n",
              "      <td>-0.278895</td>\n",
              "      <td>0.116706</td>\n",
              "      <td>-0.031764</td>\n",
              "      <td>-0.083000</td>\n",
              "      <td>-0.164207</td>\n",
              "      <td>0.186969</td>\n",
              "      <td>-0.140416</td>\n",
              "      <td>0.250234</td>\n",
              "      <td>-0.172232</td>\n",
              "      <td>-0.073691</td>\n",
              "      <td>0.108512</td>\n",
              "      <td>0.396801</td>\n",
              "      <td>0.679681</td>\n",
              "      <td>-2.680611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.093854</td>\n",
              "      <td>0.203215</td>\n",
              "      <td>0.143699</td>\n",
              "      <td>-0.192129</td>\n",
              "      <td>-0.288521</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>0.701010</td>\n",
              "      <td>0.521691</td>\n",
              "      <td>0.172952</td>\n",
              "      <td>-0.497126</td>\n",
              "      <td>-0.021346</td>\n",
              "      <td>-0.291448</td>\n",
              "      <td>-0.195253</td>\n",
              "      <td>0.582004</td>\n",
              "      <td>0.194440</td>\n",
              "      <td>0.269822</td>\n",
              "      <td>-0.013928</td>\n",
              "      <td>0.180351</td>\n",
              "      <td>0.133972</td>\n",
              "      <td>-0.020463</td>\n",
              "      <td>0.115805</td>\n",
              "      <td>-0.235973</td>\n",
              "      <td>0.051620</td>\n",
              "      <td>-0.203749</td>\n",
              "      <td>-0.006068</td>\n",
              "      <td>-0.020587</td>\n",
              "      <td>-0.031401</td>\n",
              "      <td>0.019496</td>\n",
              "      <td>-0.114106</td>\n",
              "      <td>0.145524</td>\n",
              "      <td>-0.197016</td>\n",
              "      <td>0.205308</td>\n",
              "      <td>-0.261083</td>\n",
              "      <td>-0.024287</td>\n",
              "      <td>0.115347</td>\n",
              "      <td>-0.049339</td>\n",
              "      <td>-0.160883</td>\n",
              "      <td>-0.316876</td>\n",
              "      <td>0.413236</td>\n",
              "      <td>-0.063892</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.137487</td>\n",
              "      <td>-0.212946</td>\n",
              "      <td>0.402717</td>\n",
              "      <td>-0.286000</td>\n",
              "      <td>-0.189913</td>\n",
              "      <td>-0.066704</td>\n",
              "      <td>0.174118</td>\n",
              "      <td>-0.569863</td>\n",
              "      <td>0.086228</td>\n",
              "      <td>-0.057636</td>\n",
              "      <td>0.371086</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>-0.072058</td>\n",
              "      <td>-0.066697</td>\n",
              "      <td>0.441553</td>\n",
              "      <td>0.339268</td>\n",
              "      <td>0.038720</td>\n",
              "      <td>-0.086322</td>\n",
              "      <td>0.154244</td>\n",
              "      <td>-0.308330</td>\n",
              "      <td>-0.072550</td>\n",
              "      <td>0.134763</td>\n",
              "      <td>0.397329</td>\n",
              "      <td>-6.701902</td>\n",
              "      <td>-0.170226</td>\n",
              "      <td>0.022921</td>\n",
              "      <td>-0.120816</td>\n",
              "      <td>-0.428781</td>\n",
              "      <td>-0.626987</td>\n",
              "      <td>-0.074123</td>\n",
              "      <td>-0.245851</td>\n",
              "      <td>-0.053830</td>\n",
              "      <td>0.131663</td>\n",
              "      <td>0.203021</td>\n",
              "      <td>0.052046</td>\n",
              "      <td>-0.217257</td>\n",
              "      <td>-0.185266</td>\n",
              "      <td>0.379728</td>\n",
              "      <td>0.519283</td>\n",
              "      <td>-0.956328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.175459</td>\n",
              "      <td>0.332382</td>\n",
              "      <td>-0.073295</td>\n",
              "      <td>-0.187953</td>\n",
              "      <td>-0.489943</td>\n",
              "      <td>-0.281291</td>\n",
              "      <td>0.751253</td>\n",
              "      <td>0.514160</td>\n",
              "      <td>-0.059264</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>0.007623</td>\n",
              "      <td>-0.074470</td>\n",
              "      <td>-0.115274</td>\n",
              "      <td>0.524753</td>\n",
              "      <td>0.484866</td>\n",
              "      <td>0.238129</td>\n",
              "      <td>0.046454</td>\n",
              "      <td>0.540293</td>\n",
              "      <td>0.261595</td>\n",
              "      <td>0.044107</td>\n",
              "      <td>0.038626</td>\n",
              "      <td>0.018876</td>\n",
              "      <td>0.184113</td>\n",
              "      <td>-0.025048</td>\n",
              "      <td>-0.022937</td>\n",
              "      <td>0.012909</td>\n",
              "      <td>-0.021078</td>\n",
              "      <td>-0.033133</td>\n",
              "      <td>-0.053851</td>\n",
              "      <td>0.241292</td>\n",
              "      <td>0.169756</td>\n",
              "      <td>-0.005172</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>-0.029453</td>\n",
              "      <td>0.278412</td>\n",
              "      <td>-0.153979</td>\n",
              "      <td>-0.218413</td>\n",
              "      <td>-0.244795</td>\n",
              "      <td>0.249539</td>\n",
              "      <td>-0.057608</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052503</td>\n",
              "      <td>-0.200642</td>\n",
              "      <td>0.589456</td>\n",
              "      <td>-0.847173</td>\n",
              "      <td>-0.134977</td>\n",
              "      <td>0.219423</td>\n",
              "      <td>0.020818</td>\n",
              "      <td>-0.552905</td>\n",
              "      <td>-0.038738</td>\n",
              "      <td>-0.090613</td>\n",
              "      <td>0.147529</td>\n",
              "      <td>0.388691</td>\n",
              "      <td>-0.169955</td>\n",
              "      <td>0.010452</td>\n",
              "      <td>0.318906</td>\n",
              "      <td>0.495270</td>\n",
              "      <td>-0.077713</td>\n",
              "      <td>-0.147221</td>\n",
              "      <td>0.037615</td>\n",
              "      <td>-0.026333</td>\n",
              "      <td>0.153772</td>\n",
              "      <td>0.050382</td>\n",
              "      <td>0.275281</td>\n",
              "      <td>-7.260122</td>\n",
              "      <td>-0.039380</td>\n",
              "      <td>-0.160293</td>\n",
              "      <td>-0.094471</td>\n",
              "      <td>-0.259358</td>\n",
              "      <td>-0.111941</td>\n",
              "      <td>-0.309998</td>\n",
              "      <td>-0.013568</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>-0.120220</td>\n",
              "      <td>0.554974</td>\n",
              "      <td>0.009258</td>\n",
              "      <td>0.009079</td>\n",
              "      <td>0.057054</td>\n",
              "      <td>0.363188</td>\n",
              "      <td>0.728043</td>\n",
              "      <td>-2.341075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.109969</td>\n",
              "      <td>0.171826</td>\n",
              "      <td>0.260536</td>\n",
              "      <td>0.087237</td>\n",
              "      <td>-0.108688</td>\n",
              "      <td>-0.360521</td>\n",
              "      <td>0.648624</td>\n",
              "      <td>0.754591</td>\n",
              "      <td>-0.139784</td>\n",
              "      <td>-0.434686</td>\n",
              "      <td>0.072301</td>\n",
              "      <td>-0.015631</td>\n",
              "      <td>-0.328584</td>\n",
              "      <td>0.252345</td>\n",
              "      <td>0.580037</td>\n",
              "      <td>0.265758</td>\n",
              "      <td>-0.218179</td>\n",
              "      <td>0.467285</td>\n",
              "      <td>0.047808</td>\n",
              "      <td>0.172819</td>\n",
              "      <td>0.016759</td>\n",
              "      <td>-0.538792</td>\n",
              "      <td>0.246859</td>\n",
              "      <td>-0.066110</td>\n",
              "      <td>0.033810</td>\n",
              "      <td>-0.130635</td>\n",
              "      <td>-0.019634</td>\n",
              "      <td>-0.243074</td>\n",
              "      <td>0.135270</td>\n",
              "      <td>0.099332</td>\n",
              "      <td>-0.328880</td>\n",
              "      <td>0.062511</td>\n",
              "      <td>-0.151040</td>\n",
              "      <td>0.017045</td>\n",
              "      <td>0.155072</td>\n",
              "      <td>-0.275657</td>\n",
              "      <td>-0.219200</td>\n",
              "      <td>-0.192958</td>\n",
              "      <td>0.489210</td>\n",
              "      <td>0.298175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.114253</td>\n",
              "      <td>-0.005308</td>\n",
              "      <td>0.524833</td>\n",
              "      <td>-0.103920</td>\n",
              "      <td>0.177931</td>\n",
              "      <td>-0.057709</td>\n",
              "      <td>0.053240</td>\n",
              "      <td>-0.417177</td>\n",
              "      <td>0.032487</td>\n",
              "      <td>0.096229</td>\n",
              "      <td>-0.002384</td>\n",
              "      <td>0.317829</td>\n",
              "      <td>0.116646</td>\n",
              "      <td>-0.119399</td>\n",
              "      <td>0.429504</td>\n",
              "      <td>0.421311</td>\n",
              "      <td>0.233301</td>\n",
              "      <td>-0.075609</td>\n",
              "      <td>0.242171</td>\n",
              "      <td>-0.245735</td>\n",
              "      <td>-0.133638</td>\n",
              "      <td>0.101578</td>\n",
              "      <td>0.153715</td>\n",
              "      <td>-6.844507</td>\n",
              "      <td>-0.030501</td>\n",
              "      <td>-0.194800</td>\n",
              "      <td>-0.229728</td>\n",
              "      <td>-0.340742</td>\n",
              "      <td>-0.495845</td>\n",
              "      <td>0.150437</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.104447</td>\n",
              "      <td>0.055117</td>\n",
              "      <td>0.174193</td>\n",
              "      <td>0.020948</td>\n",
              "      <td>-0.113725</td>\n",
              "      <td>-0.126994</td>\n",
              "      <td>0.264167</td>\n",
              "      <td>0.606923</td>\n",
              "      <td>-1.698191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...       766       767  sentiment_score\n",
              "0  0.064185  0.116251  0.173751  ...  0.339752  0.522551        -2.150020\n",
              "1  0.173912  0.023234 -0.282618  ...  0.396801  0.679681        -2.680611\n",
              "2 -0.093854  0.203215  0.143699  ...  0.379728  0.519283        -0.956328\n",
              "3  0.175459  0.332382 -0.073295  ...  0.363188  0.728043        -2.341075\n",
              "4  0.109969  0.171826  0.260536  ...  0.264167  0.606923        -1.698191\n",
              "\n",
              "[5 rows x 769 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEiWSJj9Ad_g",
        "colab_type": "text"
      },
      "source": [
        "Evaluating  Logistic Regression Classifier on the features obtained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc6tW4NFDxBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3046b37-a779-4d12-88b9-493b7dd29273"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features_df, labels)\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=5000)\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "score = lr_clf.score(test_features, test_labels)\n",
        "print(\"Accuracy without scaling is: \",score)\n",
        "from sklearn import preprocessing\n",
        "# Scale\n",
        "scaled_train_features = preprocessing.scale(train_features)\n",
        "scaled_test_features = preprocessing.scale(test_features)\n",
        "\n",
        "lr_clf_scaled = LogisticRegression(max_iter=5000)\n",
        "lr_clf_scaled.fit(scaled_train_features, train_labels)\n",
        "score = lr_clf_scaled.score(scaled_test_features, test_labels)\n",
        "print(\"Accuracy with scaling is: \",score)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy without scaling is:  0.869198312236287\n",
            "Accuracy with scaling is:  0.8734177215189873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUyYrzlkNR4I",
        "colab_type": "text"
      },
      "source": [
        "As we can see from above scaling the features barely affects the accuracy and hence we drop scaling moving forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkPSZ-JaArNj",
        "colab_type": "text"
      },
      "source": [
        "Evaluating Logistic Regression Classifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOgZYkAQ10RA",
        "colab_type": "code",
        "outputId": "0cc92cf1-8415-422d-dc63-bb72211840db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def unbiased_test_score(test_set,lr_clf_sentiment,model=lr_clf,return_mode=False):\n",
        "  test = vectorizeBERT(test_set)\n",
        "  test_l = test_set[\"label\"]\n",
        "  test_score = lr_clf_sentiment.predict_log_proba(test)[:,0]\n",
        "  test_df = pd.DataFrame(test)\n",
        "  test_df[\"sentiment_score\"] = test_score\n",
        "  print(\"Score for Random unbiased Dataset\",model.score(test_df, test_l))\n",
        "  if return_mode:\n",
        "    return model.predict(test_df),model.predict_proba(test_df)[:,0],test_l\n",
        "\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,lr_clf)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1LVSgyPQcHV",
        "colab_type": "text"
      },
      "source": [
        "Evaluating Linear SVM on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PucNd_ctD_pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2aa4700c-2ec9-4fc9-b7bc-05a2a93ce2d8"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "our_svm_model = SVC(kernel = \"linear\",C=1000,random_state=1)\n",
        "our_svm_model.fit(train_features, train_labels)\n",
        "biased_Score = our_svm_model.score(test_features, test_labels)\n",
        "print(\"Biased score for SVM is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,our_svm_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for SVM is:  0.8860759493670886\n",
            "Score for Random unbiased Dataset 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rhliikypkXU",
        "colab_type": "text"
      },
      "source": [
        "Evaluating RandomForestClassifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbaKPrO7j-ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "55182b90-11c0-471a-e485-48f9ae6dd590"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "modelRFC = RandomForestClassifier()\n",
        "modelRFC.fit(train_features, train_labels)\n",
        "biased_Score = modelRFC.score(test_features, test_labels)\n",
        "print(\"Biased score for RandomForestClassifier is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,modelRFC)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for RandomForestClassifier is:  0.8734177215189873\n",
            "Score for Random unbiased Dataset 0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atbu5dAYQhgw",
        "colab_type": "text"
      },
      "source": [
        "Evaluating XGBoostClassifier on a Random Test Set not a part of the augmented dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXvgDId-NqIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a728d63e-0091-411f-d2a5-206bfa3de5f6"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "modelXGB = XGBClassifier()\n",
        "modelXGB.fit(train_features, train_labels)\n",
        "biased_Score = modelXGB.score(test_features, test_labels)\n",
        "print(\"Biased score for XGBoostClassifier is: \",biased_Score)\n",
        "unbiased_test_score(test_set,lr_clf_sentiment,modelXGB)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Biased score for XGBoostClassifier is:  0.8860759493670886\n",
            "Score for Random unbiased Dataset 0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeHQkDXuit8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(best_clf.cv_results_).to_csv(\"results.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvJVY9nOBbiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "965afe9d-cb0f-4d82-91f9-a228a31bc7bf"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
        "\n",
        "# Create param grid.\n",
        "\n",
        "param_grid = [\n",
        "    {'classifier' : [LogisticRegression(max_iter=5000)],\n",
        "     'classifier__penalty' : ['l1', 'l2'],\n",
        "    'classifier__C' : np.logspace(-4, 4, 20),\n",
        "    'classifier__solver' : ['liblinear','lbfgs']},\n",
        "    {'classifier' : [RandomForestClassifier()],\n",
        "    'classifier__n_estimators' : list(range(10,101,10)),\n",
        "    'classifier__max_features' : list(range(6,32,5))},\n",
        "    {'classifier' : [SVC(probability=True)],\n",
        "    'classifier__C' : np.logspace(-4, 4, 20),\n",
        "    'classifier__kernel' : ['linear','rbf']},\n",
        "    {'classifier' : [XGBClassifier()]}\n",
        "]\n",
        "\n",
        "# Create grid search object\n",
        "\n",
        "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "\n",
        "\n",
        "best_clf = clf.fit(train_features, train_labels)\n",
        "\n",
        "\n",
        "print('Best parameters are: ',best_clf.best_params_)\n",
        "print('Best score is: ',best_clf.best_score_)\n",
        "\n",
        "pd.DataFrame(best_clf.cv_results_)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 181 candidates, totalling 905 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    6.3s\n",
            "[Parallel(n_jobs=-1)]: Done 396 tasks      | elapsed:   28.3s\n",
            "[Parallel(n_jobs=-1)]: Done 724 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done 905 out of 905 | elapsed:  4.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters are:  {'classifier': SVC(C=29.763514416313132, break_ties=False, cache_size=200, class_weight=None,\n",
            "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale',\n",
            "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
            "    shrinking=True, tol=0.001, verbose=False), 'classifier__C': 29.763514416313132, 'classifier__kernel': 'rbf'}\n",
            "Best score is:  0.902974490298434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_classifier</th>\n",
              "      <th>param_classifier__C</th>\n",
              "      <th>param_classifier__penalty</th>\n",
              "      <th>param_classifier__solver</th>\n",
              "      <th>param_classifier__max_features</th>\n",
              "      <th>param_classifier__n_estimators</th>\n",
              "      <th>param_classifier__kernel</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.036363</td>\n",
              "      <td>0.013969</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l1</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.485915</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.481011</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003037</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l1</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.030196</td>\n",
              "      <td>0.002936</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l2</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.573427</td>\n",
              "      <td>0.598592</td>\n",
              "      <td>0.612676</td>\n",
              "      <td>0.577465</td>\n",
              "      <td>0.605634</td>\n",
              "      <td>0.593559</td>\n",
              "      <td>0.015498</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.028028</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l2</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.559441</td>\n",
              "      <td>0.549296</td>\n",
              "      <td>0.591549</td>\n",
              "      <td>0.577465</td>\n",
              "      <td>0.570423</td>\n",
              "      <td>0.569635</td>\n",
              "      <td>0.014559</td>\n",
              "      <td>146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.023030</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.004031</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
              "      <td>0.000263665</td>\n",
              "      <td>l1</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': LogisticRegression(C=1.0, class...</td>\n",
              "      <td>0.482517</td>\n",
              "      <td>0.485915</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.481011</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0.842693</td>\n",
              "      <td>0.027362</td>\n",
              "      <td>0.038479</td>\n",
              "      <td>0.003932</td>\n",
              "      <td>SVC(C=29.763514416313132, break_ties=False, ca...</td>\n",
              "      <td>3792.69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>linear</td>\n",
              "      <td>{'classifier': SVC(C=29.763514416313132, break...</td>\n",
              "      <td>0.895105</td>\n",
              "      <td>0.922535</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.852113</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.884655</td>\n",
              "      <td>0.023463</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>1.147630</td>\n",
              "      <td>0.102101</td>\n",
              "      <td>0.046747</td>\n",
              "      <td>0.004232</td>\n",
              "      <td>SVC(C=29.763514416313132, break_ties=False, ca...</td>\n",
              "      <td>3792.69</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'classifier': SVC(C=29.763514416313132, break...</td>\n",
              "      <td>0.895105</td>\n",
              "      <td>0.950704</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.900148</td>\n",
              "      <td>0.028630</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>0.841886</td>\n",
              "      <td>0.029781</td>\n",
              "      <td>0.035627</td>\n",
              "      <td>0.001746</td>\n",
              "      <td>SVC(C=29.763514416313132, break_ties=False, ca...</td>\n",
              "      <td>10000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>linear</td>\n",
              "      <td>{'classifier': SVC(C=29.763514416313132, break...</td>\n",
              "      <td>0.895105</td>\n",
              "      <td>0.922535</td>\n",
              "      <td>0.880282</td>\n",
              "      <td>0.852113</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.884655</td>\n",
              "      <td>0.023463</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1.171069</td>\n",
              "      <td>0.099468</td>\n",
              "      <td>0.044243</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>SVC(C=29.763514416313132, break_ties=False, ca...</td>\n",
              "      <td>10000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'classifier': SVC(C=29.763514416313132, break...</td>\n",
              "      <td>0.895105</td>\n",
              "      <td>0.950704</td>\n",
              "      <td>0.908451</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.873239</td>\n",
              "      <td>0.900148</td>\n",
              "      <td>0.028630</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>3.761524</td>\n",
              "      <td>0.495180</td>\n",
              "      <td>0.016979</td>\n",
              "      <td>0.002944</td>\n",
              "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{'classifier': XGBClassifier(base_score=0.5, b...</td>\n",
              "      <td>0.881119</td>\n",
              "      <td>0.922535</td>\n",
              "      <td>0.830986</td>\n",
              "      <td>0.795775</td>\n",
              "      <td>0.788732</td>\n",
              "      <td>0.843829</td>\n",
              "      <td>0.051176</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0         0.036363      0.013969  ...        0.002830              157\n",
              "1         0.003037      0.000170  ...             NaN              163\n",
              "2         0.030196      0.002936  ...        0.015498              145\n",
              "3         0.028028      0.005011  ...        0.014559              146\n",
              "4         0.023030      0.002262  ...        0.002830              157\n",
              "..             ...           ...  ...             ...              ...\n",
              "176       0.842693      0.027362  ...        0.023463               31\n",
              "177       1.147630      0.102101  ...        0.028630                2\n",
              "178       0.841886      0.029781  ...        0.023463               31\n",
              "179       1.171069      0.099468  ...        0.028630                2\n",
              "180       3.761524      0.495180  ...        0.051176               85\n",
              "\n",
              "[181 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmE5QmKIbmMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump(best_clf, open(\"best_clf_bert\", 'wb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhCcK_7UC-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3bd9462-1350-4971-f3ee-486013579fc9"
      },
      "source": [
        "unbiased_test_score(test_set,lr_clf_sentiment,best_clf)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Btju4uR1Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluation_parameters(y_test,y_pred):\n",
        "  con_mat = confusion_matrix(y_test,y_pred)\n",
        "  tn, fp, fn, tp = con_mat.ravel()\n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  f1 = 2*precision*recall/(precision+recall)\n",
        "  acc = (tp+tn)/(tn+fn+fp+tp)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(con_mat)\n",
        "  print(\"The precision is: \",precision)\n",
        "  print(\"The recall is: \",recall)\n",
        "  print(\"The f1_score is: \",f1)\n",
        "  print(\"The accuracy is: \",acc)\n",
        "\n",
        "def plot_roc(y_test,y_score):\n",
        "  fpr,tpr,thresh=roc_curve(y_test,y_score)\n",
        "  plt.plot(fpr,tpr)\n",
        "  plt.grid()\n",
        "  plt.ylim(0,1.1)\n",
        "  print(\"The roc score is \",roc_auc_score(y_test,y_score))\n",
        "  plt.xlabel(\"Specificity\")\n",
        "  plt.ylabel(\"Sensitivity\")\n",
        "  plt.title(\"ROC Curve\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeignRGpSHHk",
        "colab_type": "code",
        "outputId": "cd48b91f-8d6a-4bb5-dda1-9a23e3b5cc6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred,y_score,y_test = unbiased_test_score(test_set,lr_clf_sentiment,best_clf,True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for Random unbiased Dataset 0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ET29ddLYFUi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a7ba64f9-deb4-4b43-f607-aca3b0f4a3fb"
      },
      "source": [
        "evaluation_parameters(y_test,y_pred)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[20  4]\n",
            " [ 2 24]]\n",
            "The precision is:  0.8571428571428571\n",
            "The recall is:  0.9230769230769231\n",
            "The f1_score is:  0.888888888888889\n",
            "The accuracy is:  0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qerybn2JYKtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "4bfc715d-dd2b-4d56-c48a-23fd5d3374d9"
      },
      "source": [
        "plot_roc(y_test,y_score)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The roc score is  0.11538461538461539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}